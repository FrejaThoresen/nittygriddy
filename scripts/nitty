import ctypes
from datetime import datetime
from glob import glob
from pprint import pprint
import subprocess
import argparse
import os
import shutil
import sys

import ROOT

from nittygriddy import utils, settings


# collect processes to be able to kill them
procs = []

class formatter_class(argparse.ArgumentDefaultsHelpFormatter,
                      argparse.RawTextHelpFormatter):
    pass

parser = argparse.ArgumentParser(formatter_class=formatter_class)
parser.add_argument(
    '-v', '--verbose', action='store_true', default=False)
subparsers = parser.add_subparsers()


def datasets(args):
    if args.list:
        pprint(utils.get_datasets())
    elif args.show:
        ds = utils.get_datasets().get(args.show, None)
        if not ds:
            raise ValueError("Dataset not found.")
        pprint(ds)
    elif args.download:
        if not args.volume:
            raise ValueError("No download volume specified")
        elif not utils.get_datasets().get(args.download, False):
            raise ValueError("Dataset not found.")
        utils.download(args.download, args.volume)


parser_datasets = subparsers.add_parser('datasets')
parser_datasets.add_argument(
    '-l', '--list',
    help="list all predifined datasets", default=False, action='store_true')
parser_datasets.add_argument(
    '-s', '--show',
    help="show only given dataset", type=str)
parser_datasets.add_argument(
    '-d', '--download',
    help="Download subset of specified dataset (see --volume)", type=str)
parser_datasets.add_argument(
    '--volume',
    help="Use with --download; Download this many GB", type=float)
parser_datasets.set_defaults(op=datasets)


def prepare_par_files(args, output_dir):
    """
    Copy par files to output_dir, if any are required.
    """
    par_dir = os.path.expandvars("$ALICE_PHYSICS/PARfiles/")
    for par_file in args.par_files.split(","):
        # make sure the is a .par extension
        par_file = os.path.splitext(par_file)[0] + ".par"
        shutil.copy(os.path.join(par_dir, par_file), output_dir)


def run(args):
    if not os.path.isfile(os.path.join(os.path.abspath(os.path.curdir), "nittygriddy.json"))\
       or not os.path.isfile(os.path.join(os.path.abspath(os.path.curdir), "ConfigureWagon.C")):
        print "Can only run from a nittygriddy project folder"
        return
    output_dir = os.path.join(os.path.abspath(os.path.curdir), datetime.now().strftime("%Y%m%d_%H%M"))
    if args.folder_postfix:
        output_dir += args.folder_postfix
    try:
        os.mkdir(output_dir)
    except OSError:
        print "Cannot create output folder {}".format(output_dir)
        return
    try:
        os.symlink(output_dir, "latest")
    except OSError as e:
        if os.path.islink("latest"):
            os.remove("latest")
            os.symlink(output_dir, "latest")
        else:
            raise e
    utils.copy_template_files_to(output_dir)
    shutil.copy(os.path.join(os.path.dirname(output_dir), "ConfigureWagon.C"), output_dir)
    prepare_par_files(args, output_dir)
    # generate input file
    ds = utils.get_datasets()[args.dataset]
    # create GetSetting.C in output dir (from template)
    with open(os.path.join(output_dir, "GetSetting.C"), "w") as get_setting_c:
        as_string = utils.get_template_GetSetting().\
            format(overwrite_oadb_period=ds.get("overwrite_oadb_period", ""),
                   workdir=os.path.split(output_dir)[1],
                   datadir=ds['datadir'],
                   data_pattern=ds['data_pattern'],
                   run_number_prefix=ds['run_number_prefix'],
                   run_list=args.run_list if args.run_list else ds['run_list'],
                   is_mc=ds["is_mc"],
                   datatype=ds["datatype"],
                   runmode=args.runmode,
                   nworkers=args.nworkers,
                   wait_for_gdb="true" if args.wait_for_gdb else "false",
                   aliphysics_version=utils.get_latest_aliphysics(),
                   par_files=args.par_files if args.par_files else "")
        get_setting_c.write(as_string)

    # start the analysis
    os.chdir(output_dir)
    if args.runmode != "grid":
        # create list of local files
        with open(os.path.join(output_dir, "input_files.dat"), 'a') as input_files:
            search_string = os.path.expanduser(os.path.join(settings["local_data_dir"],
                                                            ds["datadir"].lstrip("/"),
                                                            "*",
                                                            ds["data_pattern"]))
            input_files.write('\n'.join(glob(search_string)) + '\n')
        # command to start the analysis
        cmd = ['root', '-l', '-q', 'run.C']
    else:
        cmd = ['root', '-l', '-q', '-x', 'run.C(\"full\")']
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    procs.append(p)
    for line in iter(p.stdout.readline, b''):
        print(line.rstrip())  # rstrip to remove \n; doesn't like carriage returns

description_run = """Start analysis on target platform. Must be executed from a
nittygriddy project folder (ie. next to a nittygriddy.json file)"""
parser_run = subparsers.add_parser('run', description=description_run)
parser_run.add_argument('runmode', choices=('local', 'lite', 'grid'))
parser_run.add_argument('dataset', type=str, help="Use this dataset")
parser_run.add_argument('--folder_postfix', type=str, help="Attach to the end of the folder name")
parser_run.add_argument('--nworkers', type=str, help="Number of workers for proof lite", default="-1")
parser_run.add_argument('--par_files', type=str, help="Patch aliphysics on the grid with par this file. Build par_files before with cd $ALICE_ROOT/../build; make MODULE.par; make -j$MJ install")
parser_run.add_argument('--run_list', type=str, help="Overwrite default (comma seperated) run list for the given dataset (grid only)")
parser_run.add_argument('--wait_for_gdb', action='store_true', default=False, help="Pause the execution to allow for connecting gdb to the process")
parser_run.set_defaults(op=run)


def merge(args):
    # check if we are in a output dirname
    if not (os.path.isfile("./ConfigureWagon.C") and
            os.path.isfile("./GetSetting.C") and
            os.path.isfile("./run.C")):
        raise ValueError("This command needs to be run from an output directory")
    ROOT.gROOT.LoadMacro(r'GetSetting.C')
    if ctypes.c_char_p(ROOT.gROOT.ProcessLine(r'GetSetting("runmode").c_str()')).value != "grid":
        raise ValueError("The data in this folder was not run on the grid!")
    if args.mergemode == 'online':
        cmd = r'root -l -q -x "run.C(\"merge_online\")"'
    elif args.mergemode == 'offline':
        cmd = r'root -l -q -x "run.C(\"merge_offline\")"'
    # root does not like it if the stdout is piped and it uses some shell functionality
    # thus, using call on the whole string and shell True
    subprocess.check_call(cmd, shell=True)

description_merge = """Merge the output of a previously run grid analysis. Must be
executed from output folder of that analysis."""
parser_merge = subparsers.add_parser('merge', description=description_merge)
parser_merge.add_argument('mergemode', choices=('online', 'offline'))
parser_merge.set_defaults(op=merge)


args = parser.parse_args()
try:
    args.op(args)
except KeyboardInterrupt:
    for proc in procs:
        print "Killing: ", proc
        proc.terminate()
    sys.exit(1)
except Exception as e:
    if args.verbose:
        # re-raise with the full traceback
        t, v, tb = sys.exc_info()
        raise t, v, tb
    else:
        sys.exit("{0}: {1}".format(e.__class__.__name__, e))
